{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('C:/Users/Acer/Documents/Fusemachines/LayoutLM/dataset/train.json') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qas'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['qas'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "example = data['qas'][10]\n",
    "\n",
    "root_dir = 'C:/Users/Acer/Documents/Fusemachines/LayoutLM/dataset/'\n",
    "image = Image.open(root_dir + \"documents/\" + example['image'])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in example.items():\n",
    "  print(k + \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_root_dir = root_dir + \"ocr_result/\"\n",
    "\n",
    "with open(ocr_root_dir + example['image'] [:-3] + \"json\") as f:\n",
    "  ocr = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['height', 'width', 'lines'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''\n",
    "for item in ocr['lines']:\n",
    "    words += item['text']\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\layoutlmvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df.iloc[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\layoutlmvenv\\Lib\\site-packages\\transformers\\models\\layoutlmv2\\feature_extraction_layoutlmv2.py:30: FutureWarning: The class LayoutLMv2FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv2ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2FeatureExtractor\n",
    "\n",
    "feature_extractor = LayoutLMv2FeatureExtractor()\n",
    "\n",
    "def get_ocr_words_and_boxes(examples):\n",
    "    \n",
    "  images = [Image.open(root_dir + \"documents/\" + image_file).convert(\"RGB\") for image_file in examples['image']]\n",
    "  \n",
    "  # resize every image to 224x224 + apply tesseract to get words + normalized boxes\n",
    "  encoded_inputs = feature_extractor(images)\n",
    "\n",
    "  examples['image'] = encoded_inputs.pixel_values\n",
    "  examples['words'] = encoded_inputs.words\n",
    "  examples['boxes'] = encoded_inputs.boxes\n",
    "\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [02:04<00:00,  2.49s/ examples]\n"
     ]
    }
   ],
   "source": [
    "dataset_with_ocr = dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_with_ocr[0]['words'])\n",
    "print(dataset_with_ocr[0]['boxes'])\n",
    "print(\"-----\")\n",
    "print(dataset_with_ocr[5]['words'])\n",
    "print(dataset_with_ocr[5]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_id', 'question', 'answers', 'image', 'words', 'boxes'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_ocr[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfinder(words_list, answer_list):  \n",
    "    matches = []\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for idx, i in enumerate(range(len(words_list))):\n",
    "        if words_list[i] == answer_list[0] and words_list[i:i+len(answer_list)] == answer_list:\n",
    "            matches.append(answer_list)\n",
    "            start_indices.append(idx)\n",
    "            end_indices.append(idx + len(answer_list) - 1)\n",
    "    if matches:\n",
    "      return matches[0], start_indices[0], end_indices[0]\n",
    "    else:\n",
    "      return None, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where is it located?\"\n",
    "words = [\"this\", \"is\", \"located\", \"in\", \"the\", \"university\", \"of\", \"california\", \"in\", \"the\", \"US\"]\n",
    "boxes = [[1000,1000,1000,1000] for _ in range(len(words))]\n",
    "answer = \"university of california\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(question, words, boxes=boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] where is it located? [SEP] this is located in the university of california in the us [SEP]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.sequence_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "match, word_idx_start, word_idx_end = subfinder(words, answer.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['university', 'of', 'california']\n",
      "Word idx start: 5\n",
      "Word idx end: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Match:\", match)\n",
    "print(\"Word idx start:\", word_idx_start)\n",
    "print(\"Word idx end:\", word_idx_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token start index: 7\n",
      "Token end index: 17\n",
      "this is located in the university of california in the us\n",
      "Word ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "12\n",
      "14\n",
      "Reconstructed answer: university of california\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = encoding.sequence_ids()\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(encoding.input_ids) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "print(\"Token start index:\", token_start_index)\n",
    "print(\"Token end index:\", token_end_index)\n",
    "print(tokenizer.decode(encoding.input_ids[token_start_index:token_end_index+1]))\n",
    "\n",
    "word_ids = encoding.word_ids()[token_start_index:token_end_index+1]\n",
    "print(\"Word ids:\", word_ids)\n",
    "for id in word_ids:\n",
    "  if id == word_idx_start:\n",
    "    start_position = token_start_index \n",
    "  else:\n",
    "    token_start_index += 1\n",
    "\n",
    "for id in word_ids[::-1]:\n",
    "  if id == word_idx_end:\n",
    "    end_position = token_end_index \n",
    "  else:\n",
    "    token_end_index -= 1\n",
    "\n",
    "print(start_position)\n",
    "print(end_position)\n",
    "print(\"Reconstructed answer:\", tokenizer.decode(encoding.input_ids[start_position:end_position+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(examples, max_length=512):\n",
    "  # take a batch \n",
    "  questions = examples['question']\n",
    "  words = examples['words']\n",
    "  boxes = examples['boxes']\n",
    "\n",
    "  # encode it\n",
    "  encoding = tokenizer(questions, words, boxes, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "  # next, add start_positions and end_positions\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  answers = examples['answers']\n",
    "  # for every example in the batch:\n",
    "  for batch_index in range(len(answers)):\n",
    "    print(\"Batch index:\", batch_index)\n",
    "    cls_index = encoding.input_ids[batch_index].index(tokenizer.cls_token_id)\n",
    "    # try to find one of the answers in the context, return first match\n",
    "    words_example = [word.lower() for word in words[batch_index]]\n",
    "    for answer in answers[batch_index]:\n",
    "      match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())\n",
    "      if match:\n",
    "        break\n",
    "    # EXPERIMENT (to account for when OCR context and answer don't perfectly match):\n",
    "    if not match:\n",
    "      for answer in answers[batch_index]:\n",
    "        for i in range(len(answer)):\n",
    "          # drop the ith character from the answer\n",
    "          answer_i = answer[:i] + answer[i+1:]\n",
    "          # check if we can find this one in the context\n",
    "          match, word_idx_start, word_idx_end = subfinder(words_example, answer_i.lower().split())\n",
    "          if match:\n",
    "            break\n",
    "    # END OF EXPERIMENT \n",
    "    \n",
    "    if match:\n",
    "      sequence_ids = encoding.sequence_ids(batch_index)\n",
    "      # Start token index of the current span in the text.\n",
    "      token_start_index = 0\n",
    "      while sequence_ids[token_start_index] != 1:\n",
    "          token_start_index += 1\n",
    "\n",
    "      # End token index of the current span in the text.\n",
    "      token_end_index = len(encoding.input_ids[batch_index]) - 1\n",
    "      while sequence_ids[token_end_index] != 1:\n",
    "          token_end_index -= 1\n",
    "      \n",
    "      word_ids = encoding.word_ids(batch_index)[token_start_index:token_end_index+1]\n",
    "      for id in word_ids:\n",
    "        if id == word_idx_start:\n",
    "          start_positions.append(token_start_index)\n",
    "          break\n",
    "        else:\n",
    "          token_start_index += 1\n",
    "\n",
    "      for id in word_ids[::-1]:\n",
    "        if id == word_idx_end:\n",
    "          end_positions.append(token_end_index)\n",
    "          break\n",
    "        else:\n",
    "          token_end_index -= 1\n",
    "      \n",
    "      print(\"Verifying start position and end position:\")\n",
    "      print(\"True answer:\", answer)\n",
    "      start_position = start_positions[batch_index]\n",
    "      end_position = end_positions[batch_index]\n",
    "      reconstructed_answer = tokenizer.decode(encoding.input_ids[batch_index][start_position:end_position+1])\n",
    "      print(\"Reconstructed answer:\", reconstructed_answer)\n",
    "      print(\"-----------\")\n",
    "    \n",
    "    else:\n",
    "      print(\"Answer not found in context\")\n",
    "      print(\"-----------\")\n",
    "      start_positions.append(cls_index)\n",
    "      end_positions.append(cls_index)\n",
    "  \n",
    "  encoding['image'] = examples['image']\n",
    "  encoding['start_positions'] = start_positions\n",
    "  encoding['end_positions'] = end_positions\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_with_ocr['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Sequence, Value, Array2D, Array3D\n",
    "\n",
    "# we need to define custom features\n",
    "features = Features({\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
    "    'start_positions': Value(dtype='int64'),\n",
    "    'end_positions': Value(dtype='int64'),\n",
    "})\n",
    "\n",
    "encoded_dataset = dataset_with_ocr.map(encode_dataset, batched=True, batch_size=2, \n",
    "                                       remove_columns=dataset_with_ocr.column_names,\n",
    "                                       features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'start_positions', 'end_positions'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'start_positions', 'end_positions'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 44\n",
    "\n",
    "tokenizer.decode(encoded_dataset['input_ids'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['image'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('C:/Users/Acer/Documents/Fusemachines/LayoutLM/dataset/documents/' + dataset['image'][idx])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 - 0009718\n"
     ]
    }
   ],
   "source": [
    "start_position = encoded_dataset['start_positions'][idx]\n",
    "end_position = encoded_dataset['end_positions'][idx]\n",
    "if start_position != 0:\n",
    "  print(tokenizer.decode(encoded_dataset['input_ids'][idx][start_position: end_position+1]))\n",
    "else:\n",
    "  print(\"Answer not found in context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'bbox': Array2D(shape=(512, 4), dtype='int64', id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'image': Array3D(shape=(3, 224, 224), dtype='int64', id=None),\n",
       " 'start_positions': Value(dtype='int64', id=None),\n",
       " 'end_positions': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "attention_mask torch.Size([4, 512])\n",
      "token_type_ids torch.Size([4, 512])\n",
      "image torch.Size([4, 3, 224, 224])\n",
      "start_positions torch.Size([4])\n",
      "end_positions torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encoded_dataset.set_format(type=\"torch\")\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=4)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "for k,v in batch.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023 - 0009658'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_position = batch['start_positions'][idx].item()\n",
    "end_position = batch['end_positions'][idx].item()\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][idx][start_position:end_position+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/layoutlmv2-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\layoutlmvenv\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Acer\\anaconda3\\envs\\layoutlmvenv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Some weights of LayoutLMv2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['layoutlmv2.visual_segment_embedding', 'qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\layoutlmvenv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.245140075683594\n",
      "Loss: 6.24925422668457\n",
      "Loss: 6.1637163162231445\n",
      "Loss: 6.036060333251953\n",
      "Loss: 6.2166547775268555\n",
      "Loss: 5.975975036621094\n",
      "Loss: 5.965182304382324\n",
      "Loss: 5.935624122619629\n",
      "Loss: 6.0863938331604\n",
      "Loss: 5.713127613067627\n",
      "Loss: 5.720980167388916\n",
      "Loss: 5.600113391876221\n",
      "Loss: 5.785351753234863\n",
      "Loss: 5.413821220397949\n",
      "Loss: 5.594992637634277\n",
      "Loss: 5.133309364318848\n",
      "Loss: 5.099974632263184\n",
      "Loss: 5.306059837341309\n",
      "Loss: 4.982330799102783\n",
      "Loss: 4.888637542724609\n",
      "Loss: 4.8870158195495605\n",
      "Loss: 4.954538345336914\n",
      "Loss: 4.551721572875977\n",
      "Loss: 4.6879682540893555\n",
      "Loss: 4.406125068664551\n",
      "Loss: 4.6027421951293945\n",
      "Loss: 4.233583927154541\n",
      "Loss: 5.174217224121094\n",
      "Loss: 4.193729400634766\n",
      "Loss: 4.274009704589844\n",
      "Loss: 4.46563720703125\n",
      "Loss: 4.168525695800781\n",
      "Loss: 4.12448787689209\n",
      "Loss: 4.149589538574219\n",
      "Loss: 4.568869113922119\n",
      "Loss: 3.9291772842407227\n",
      "Loss: 4.086633682250977\n",
      "Loss: 3.9285855293273926\n",
      "Loss: 4.152186393737793\n",
      "Loss: 3.874512195587158\n",
      "Loss: 4.472742080688477\n",
      "Loss: 3.8531928062438965\n",
      "Loss: 3.7800326347351074\n",
      "Loss: 4.081336975097656\n",
      "Loss: 3.741856098175049\n",
      "Loss: 4.0559611320495605\n",
      "Loss: 3.796489715576172\n",
      "Loss: 4.6601972579956055\n",
      "Loss: 3.6183152198791504\n",
      "Loss: 3.759748935699463\n",
      "Loss: 3.666567802429199\n",
      "Loss: 3.9284958839416504\n",
      "Loss: 3.6557788848876953\n",
      "Loss: 3.8202385902404785\n",
      "Loss: 3.6327672004699707\n",
      "Loss: 3.605487585067749\n",
      "Loss: 3.911980152130127\n",
      "Loss: 3.5905799865722656\n",
      "Loss: 3.690864086151123\n",
      "Loss: 3.7621355056762695\n",
      "Loss: 3.7574410438537598\n",
      "Loss: 3.4665613174438477\n",
      "Loss: 3.785451650619507\n",
      "Loss: 3.8458938598632812\n",
      "Loss: 3.6982874870300293\n",
      "Loss: 3.538538932800293\n",
      "Loss: 3.5466129779815674\n",
      "Loss: 3.5594725608825684\n",
      "Loss: 3.526197910308838\n",
      "Loss: 3.8230037689208984\n",
      "Loss: 3.4681005477905273\n",
      "Loss: 3.594883441925049\n",
      "Loss: 3.6207430362701416\n",
      "Loss: 3.594792366027832\n",
      "Loss: 3.385404109954834\n",
      "Loss: 3.508103370666504\n",
      "Loss: 3.5188069343566895\n",
      "Loss: 3.8489739894866943\n",
      "Loss: 3.3172125816345215\n",
      "Loss: 3.5236659049987793\n",
      "Loss: 3.428290843963623\n",
      "Loss: 3.4524545669555664\n",
      "Loss: 3.866515636444092\n",
      "Loss: 3.5765609741210938\n",
      "Loss: 3.719616651535034\n",
      "Loss: 3.7847962379455566\n",
      "Loss: 3.711273193359375\n",
      "Loss: 3.3193583488464355\n",
      "Loss: 3.4838414192199707\n",
      "Loss: 3.36590576171875\n",
      "Loss: 3.6101138591766357\n",
      "Loss: 3.294506549835205\n",
      "Loss: 3.3045454025268555\n",
      "Loss: 4.019715785980225\n",
      "Loss: 3.1925511360168457\n",
      "Loss: 3.4950320720672607\n",
      "Loss: 3.1604361534118652\n",
      "Loss: 3.327334403991699\n",
      "Loss: 3.6663599014282227\n",
      "Loss: 3.39151930809021\n",
      "Loss: 3.175335645675659\n",
      "Loss: 3.3240694999694824\n",
      "Loss: 3.2080576419830322\n",
      "Loss: 3.459646701812744\n",
      "Loss: 3.2614665031433105\n",
      "Loss: 3.262106418609619\n",
      "Loss: 3.2199525833129883\n",
      "Loss: 3.1529247760772705\n",
      "Loss: 3.5964741706848145\n",
      "Loss: 3.119063377380371\n",
      "Loss: 3.3069705963134766\n",
      "Loss: 3.3215126991271973\n",
      "Loss: 3.3067984580993652\n",
      "Loss: 3.1252620220184326\n",
      "Loss: 3.2611899375915527\n",
      "Loss: 3.1266074180603027\n",
      "Loss: 3.1951072216033936\n",
      "Loss: 3.084711790084839\n",
      "Loss: 3.501471757888794\n",
      "Loss: 3.627861499786377\n",
      "Loss: 2.9542298316955566\n",
      "Loss: 3.3042256832122803\n",
      "Loss: 2.988468647003174\n",
      "Loss: 3.1313090324401855\n",
      "Loss: 3.205646514892578\n",
      "Loss: 3.1968045234680176\n",
      "Loss: 3.000783920288086\n",
      "Loss: 3.152642011642456\n",
      "Loss: 2.979004383087158\n",
      "Loss: 3.132321357727051\n",
      "Loss: 2.9389419555664062\n",
      "Loss: 3.4318480491638184\n",
      "Loss: 3.821194648742676\n",
      "Loss: 2.89194655418396\n",
      "Loss: 3.2546958923339844\n",
      "Loss: 3.2732248306274414\n",
      "Loss: 3.000824451446533\n",
      "Loss: 3.1362903118133545\n",
      "Loss: 3.101001501083374\n",
      "Loss: 2.8574471473693848\n",
      "Loss: 3.025313377380371\n",
      "Loss: 2.8833160400390625\n",
      "Loss: 3.0051822662353516\n",
      "Loss: 2.9112863540649414\n",
      "Loss: 3.056851863861084\n",
      "Loss: 2.9066317081451416\n",
      "Loss: 2.784414291381836\n",
      "Loss: 3.064028263092041\n",
      "Loss: 2.7954273223876953\n",
      "Loss: 2.947281837463379\n",
      "Loss: 3.078944683074951\n",
      "Loss: 3.0334479808807373\n",
      "Loss: 2.8359508514404297\n",
      "Loss: 2.9519925117492676\n",
      "Loss: 2.7956371307373047\n",
      "Loss: 2.8363840579986572\n",
      "Loss: 3.0496602058410645\n",
      "Loss: 2.869661331176758\n",
      "Loss: 3.2295308113098145\n",
      "Loss: 2.653724193572998\n",
      "Loss: 2.962794065475464\n",
      "Loss: 2.7199864387512207\n",
      "Loss: 3.0102412700653076\n",
      "Loss: 2.8969836235046387\n",
      "Loss: 2.9258580207824707\n",
      "Loss: 2.6888561248779297\n",
      "Loss: 2.860956907272339\n",
      "Loss: 2.738478660583496\n",
      "Loss: 2.960000514984131\n",
      "Loss: 2.840219020843506\n",
      "Loss: 2.78468656539917\n",
      "Loss: 2.747323989868164\n",
      "Loss: 2.7234947681427\n",
      "Loss: 2.984152317047119\n",
      "Loss: 2.616823196411133\n",
      "Loss: 2.7622768878936768\n",
      "Loss: 2.809370517730713\n",
      "Loss: 2.779092788696289\n",
      "Loss: 2.635000228881836\n",
      "Loss: 2.7538931369781494\n",
      "Loss: 3.086477518081665\n",
      "Loss: 2.7482054233551025\n",
      "Loss: 2.6635494232177734\n",
      "Loss: 2.6754260063171387\n",
      "Loss: 2.6445744037628174\n",
      "Loss: 2.629486560821533\n",
      "Loss: 2.8438777923583984\n",
      "Loss: 2.521815299987793\n",
      "Loss: 2.657310724258423\n",
      "Loss: 2.679182529449463\n",
      "Loss: 4.011350631713867\n",
      "Loss: 2.5748369693756104\n",
      "Loss: 2.7388033866882324\n",
      "Loss: 2.6013877391815186\n",
      "Loss: 2.784867763519287\n",
      "Loss: 2.6732563972473145\n",
      "Loss: 2.626638174057007\n",
      "Loss: 2.6204586029052734\n",
      "Loss: 2.4898107051849365\n",
      "Loss: 2.8075618743896484\n",
      "Loss: 2.463352680206299\n",
      "Loss: 2.6065826416015625\n",
      "Loss: 2.7126264572143555\n",
      "Loss: 2.708988666534424\n",
      "Loss: 2.65340518951416\n",
      "Loss: 2.631460189819336\n",
      "Loss: 2.5465731620788574\n",
      "Loss: 2.743318557739258\n",
      "Loss: 2.5584611892700195\n",
      "Loss: 2.569705009460449\n",
      "Loss: 3.0375895500183105\n",
      "Loss: 2.4865291118621826\n",
      "Loss: 2.79057240486145\n",
      "Loss: 2.4492030143737793\n",
      "Loss: 2.6214585304260254\n",
      "Loss: 2.7301883697509766\n",
      "Loss: 2.709092617034912\n",
      "Loss: 2.426534414291382\n",
      "Loss: 2.601534605026245\n",
      "Loss: 2.5200228691101074\n",
      "Loss: 2.5107645988464355\n",
      "Loss: 2.4842357635498047\n",
      "Loss: 2.9997785091400146\n",
      "Loss: 3.3386518955230713\n",
      "Loss: 2.2954440116882324\n",
      "Loss: 2.551987648010254\n",
      "Loss: 2.359456777572632\n",
      "Loss: 2.555513381958008\n",
      "Loss: 2.5601115226745605\n",
      "Loss: 2.5994820594787598\n",
      "Loss: 2.579224109649658\n",
      "Loss: 2.6881303787231445\n",
      "Loss: 3.5253238677978516\n",
      "Loss: 2.486872911453247\n",
      "Loss: 2.5184826850891113\n",
      "Loss: 2.5978550910949707\n",
      "Loss: 2.641144275665283\n",
      "Loss: 2.321868896484375\n",
      "Loss: 2.559727668762207\n",
      "Loss: 2.320706605911255\n",
      "Loss: 2.4801902770996094\n",
      "Loss: 2.4880599975585938\n",
      "Loss: 3.0111196041107178\n",
      "Loss: 2.307612419128418\n",
      "Loss: 2.723473072052002\n",
      "Loss: 2.5017940998077393\n",
      "Loss: 2.2438127994537354\n",
      "Loss: 2.4429171085357666\n",
      "Loss: 2.399874687194824\n",
      "Loss: 2.3202054500579834\n",
      "Loss: 2.25250244140625\n",
      "Loss: 2.4809863567352295\n",
      "Loss: 2.202955961227417\n",
      "Loss: 2.402129888534546\n",
      "Loss: 2.4305853843688965\n",
      "Loss: 2.454467296600342\n",
      "Loss: 2.1594583988189697\n",
      "Loss: 2.4050440788269043\n",
      "Loss: 2.308201313018799\n",
      "Loss: 2.0697250366210938\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "   for idx, batch in enumerate(dataloader):\n",
    "        # get the inputs;\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        bbox = batch[\"bbox\"].to(device)\n",
    "        image = batch[\"image\"].to(device)\n",
    "        start_positions = batch[\"start_positions\"].to(device)\n",
    "        end_positions = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                       bbox=bbox, image=image, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutlmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
